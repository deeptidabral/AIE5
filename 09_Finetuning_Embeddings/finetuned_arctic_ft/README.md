---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:152
- loss:MatryoshkaLoss
- loss:MultipleNegativesRankingLoss
base_model: Snowflake/snowflake-arctic-embed-l
widget:
- source_sentence: 'QUESTION #1\n'
  sentences:
  - 'Google‚Äôs NotebookLM, released in September, took audio output to a new level
    by producing spookily realistic conversations between two ‚Äúpodcast hosts‚Äù about
    anything you fed into their tool. They later added custom instructions, so naturally
    I turned them into pelicans:


    The most recent twist, again from December (December was a lot) is live video.
    ChatGPT voice mode now provides the option to share your camera feed with the
    model and talk about what you can see in real time. Google Gemini have a preview
    of the same feature, which they managed to ship the day before ChatGPT did.'
  - 'Sometimes it omits sections of code and leaves you to fill them in, but if you
    tell it you can‚Äôt type because you don‚Äôt have any fingers it produces the full
    code for you instead.


    There are so many more examples like this. Offer it cash tips for better answers.
    Tell it your career depends on it. Give it positive reinforcement. It‚Äôs all so
    dumb, but it works!


    Gullibility is the biggest unsolved problem


    I coined the term prompt injection in September last year.


    15 months later, I regret to say that we‚Äôre still no closer to a robust, dependable
    solution to this problem.


    I‚Äôve written a ton about this already.


    Beyond that specific class of security vulnerabilities, I‚Äôve started seeing this
    as a wider problem of gullibility.'
  - 'Law is not ethics. Is it OK to train models on people‚Äôs content without their
    permission, when those models will then be used in ways that compete with those
    people?


    As the quality of results produced by AI models has increased over the year, these
    questions have become even more pressing.


    The impact on human society in terms of these models is already huge, if difficult
    to objectively measure.


    People have certainly lost work to them‚Äîanecdotally, I‚Äôve seen this for copywriters,
    artists and translators.


    There are a great deal of untold stories here. I‚Äôm hoping 2024 sees significant
    amounts of dedicated journalism on this topic.


    My blog in 2023'
- source_sentence: 'QUESTION #1\n'
  sentences:
  - 'I‚Äôve found myself using this a lot. I noticed how much I was relying on it in
    October and wrote Everything I built with Claude Artifacts this week, describing
    14 little tools I had put together in a seven day period.


    Since then, a whole bunch of other teams have built similar systems. GitHub announced
    their version of this‚ÄîGitHub Spark‚Äîin October. Mistral Chat added it as a feature
    called Canvas in November.


    Steve Krouse from Val Town built a version of it against Cerebras, showcasing
    how a 2,000 token/second LLM can iterate on an application with changes visible
    in less than a second.'
  - 'We already knew LLMs were spookily good at writing code. If you prompt them right,
    it turns out they can build you a full interactive application using HTML, CSS
    and JavaScript (and tools like React if you wire up some extra supporting build
    mechanisms)‚Äîoften in a single prompt.


    Anthropic kicked this idea into high gear when they released Claude Artifacts,
    a groundbreaking new feature that was initially slightly lost in the noise due
    to being described half way through their announcement of the incredible Claude
    3.5 Sonnet.


    With Artifacts, Claude can write you an on-demand interactive application and
    then let you use it directly inside the Claude interface.


    Here‚Äôs my Extract URLs app, entirely generated by Claude:'
  - 'Prompt injection is a natural consequence of this gulibility. I‚Äôve seen precious
    little progress on tackling that problem in 2024, and we‚Äôve been talking about
    it since September 2022.


    I‚Äôm beginning to see the most popular idea of ‚Äúagents‚Äù as dependent on AGI itself.
    A model that‚Äôs robust against gulliblity is a very tall order indeed.


    Evals really matter


    Anthropic‚Äôs Amanda Askell (responsible for much of the work behind Claude‚Äôs Character):


    The boring yet crucial secret behind good system prompts is test-driven development.
    You don‚Äôt write down a system prompt and find ways to test it. You write down
    tests and find a system prompt that passes them.'
- source_sentence: 'QUESTION #2\n...\n\nContext:\nOpenAI made GPT-4o free for all
    users in May, and Claude'
  sentences:
  - 'This remains astonishing to me. I thought a model with the capabilities and output
    quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.


    These models take up enough of my 64GB of RAM that I don‚Äôt run them often‚Äîthey
    don‚Äôt leave much room for anything else.


    The fact that they run at all is a testament to the incredible training and inference
    performance gains that we‚Äôve figured out over the past year. It turns out there
    was a lot of low-hanging fruit to be harvested in terms of model efficiency. I
    expect there‚Äôs still more to come.'
  - 'OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely
    available from its launch in June. This was a momentus change, because for the
    previous year free users had mostly been restricted to GPT-3.5 level models, meaning
    new users got a very inaccurate mental model of what a capable LLM could actually
    do.


    That era appears to have ended, likely permanently, with OpenAI‚Äôs launch of ChatGPT
    Pro. This $200/month subscription service is the only way to access their most
    capable model, o1 Pro.'
  - 'OpenAI are not the only game in town here. Google released their first entrant
    in the category, gemini-2.0-flash-thinking-exp, on December 19th.


    Alibaba‚Äôs Qwen team released their QwQ model on November 28th‚Äîunder an Apache
    2.0 license, and that one I could run on my own machine. They followed that up
    with a vision reasoning model called QvQ on December 24th, which I also ran locally.


    DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through
    their chat interface on November 20th.


    To understand more about inference scaling I recommend Is AI progress slowing
    down? by Arvind Narayanan and Sayash Kapoor.'
- source_sentence: 'QUESTION #2\n...\n\nContext:\nThese abilities are just a few weeks
    old at this point, and I don‚Äôt think their impact has been fully felt yet. If
    you haven‚Äôt tried them out yet you really should.\n\nBoth Gemini and OpenAI offer
    API access to these features as well. OpenAI started with a WebSocket API that
    was quite challenging to use, but in December they announced a new WebRTC API
    which is much easier to get started with. Building a web app that a user can talk
    to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis
    was possible with GPT-4 in 2023, but the value it provides became evident in'
  sentences:
  - 'On the other hand, as software engineers we are better placed to take advantage
    of this than anyone else. We‚Äôve all been given weird coding interns‚Äîwe can use
    our deep knowledge to prompt them to solve coding problems more effectively than
    anyone else can.


    The ethics of this space remain diabolically complex


    In September last year Andy Baio and I produced the first major story on the unlicensed
    training data behind Stable Diffusion.


    Since then, almost every major LLM (and most of the image generation models) have
    also been trained on unlicensed data.'
  - 'The environmental impact got much, much worse


    The much bigger problem here is the enormous competitive buildout of the infrastructure
    that is imagined to be necessary for these models in the future.


    Companies like Google, Meta, Microsoft and Amazon are all spending billions of
    dollars rolling out new datacenters, with a very material impact on the electricity
    grid and the environment. There‚Äôs even talk of spinning up new nuclear power stations,
    but those can take decades.'
  - 'These abilities are just a few weeks old at this point, and I don‚Äôt think their
    impact has been fully felt yet. If you haven‚Äôt tried them out yet you really should.


    Both Gemini and OpenAI offer API access to these features as well. OpenAI started
    with a WebSocket API that was quite challenging to use, but in December they announced
    a new WebRTC API which is much easier to get started with. Building a web app
    that a user can talk to via voice is easy now!


    Prompt driven app generation is a commodity already


    This was possible with GPT-4 in 2023, but the value it provides became evident
    in 2024.'
- source_sentence: 'QUESTION #2\n...\n\nContext:\nNothing yet from Anthropic or Meta
    but I would be very surprised if they don‚Äôt have their own inference-scaling models
    in the works. Meta published a relevant paper Training Large Language Models to
    Reason in a Continuous Latent Space in December.\n\nWas the best currently available
    LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make
    for a great attention-grabbing headline.\n\nThe big news to end the year was the
    release of DeepSeek v3‚Äîdropped on Hugging Face on Christmas Day without so much
    as a README file, then followed by documentation and a paper the day after that.\n'',
    additional_kwargs={}, response_metadata={})]'
  sentences:
  - 'Nothing yet from Anthropic or Meta but I would be very surprised if they don‚Äôt
    have their own inference-scaling models in the works. Meta published a relevant
    paper Training Large Language Models to Reason in a Continuous Latent Space in
    December.


    Was the best currently available LLM trained in China for less than $6m?


    Not quite, but almost! It does make for a great attention-grabbing headline.


    The big news to end the year was the release of DeepSeek v3‚Äîdropped on Hugging
    Face on Christmas Day without so much as a README file, then followed by documentation
    and a paper the day after that.'
  - 'Longer inputs dramatically increase the scope of problems that can be solved
    with an LLM: you can now throw in an entire book and ask questions about its contents,
    but more importantly you can feed in a lot of example code to help the model correctly
    solve a coding problem. LLM use-cases that involve long inputs are far more interesting
    to me than short prompts that rely purely on the information already baked into
    the model weights. Many of my tools were built using this pattern.'
  - "Industry‚Äôs Tardy Response to the AI Prompt Injection Vulnerability on RedMonk\
    \ Conversations\n\nPosted \n\n31st December 2023 at 11:59 pm ¬∑ Follow me on\n\n\
    Mastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\
    \nLLM 0.22, the annotated release notes - 17th February 2025\n\nRun LLMs on macOS\
    \ using llm-mlx and Apple's MLX framework - 15th February 2025\n\nURL-addressable\
    \ Pyodide Python environments - 13th February 2025\n\nThis is Stuff we figured\
    \ out about AI in 2023 by Simon Willison, posted on 31st December 2023.\n\nPart\
    \ of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec.\
    \ 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024,\
    \ 6:07 p.m.\n\nblogging\n            68\n\nai\n            1100"
pipeline_tag: sentence-similarity
library_name: sentence-transformers
metrics:
- cosine_accuracy@1
- cosine_accuracy@3
- cosine_accuracy@5
- cosine_accuracy@10
- cosine_precision@1
- cosine_precision@3
- cosine_precision@5
- cosine_precision@10
- cosine_recall@1
- cosine_recall@3
- cosine_recall@5
- cosine_recall@10
- cosine_ndcg@10
- cosine_mrr@10
- cosine_map@100
model-index:
- name: SentenceTransformer based on Snowflake/snowflake-arctic-embed-l
  results:
  - task:
      type: information-retrieval
      name: Information Retrieval
    dataset:
      name: Unknown
      type: unknown
    metrics:
    - type: cosine_accuracy@1
      value: 0.5405405405405406
      name: Cosine Accuracy@1
    - type: cosine_accuracy@3
      value: 0.5945945945945946
      name: Cosine Accuracy@3
    - type: cosine_accuracy@5
      value: 0.6486486486486487
      name: Cosine Accuracy@5
    - type: cosine_accuracy@10
      value: 0.7837837837837838
      name: Cosine Accuracy@10
    - type: cosine_precision@1
      value: 0.5405405405405406
      name: Cosine Precision@1
    - type: cosine_precision@3
      value: 0.19819819819819817
      name: Cosine Precision@3
    - type: cosine_precision@5
      value: 0.12972972972972974
      name: Cosine Precision@5
    - type: cosine_precision@10
      value: 0.07837837837837838
      name: Cosine Precision@10
    - type: cosine_recall@1
      value: 0.5405405405405406
      name: Cosine Recall@1
    - type: cosine_recall@3
      value: 0.5945945945945946
      name: Cosine Recall@3
    - type: cosine_recall@5
      value: 0.6486486486486487
      name: Cosine Recall@5
    - type: cosine_recall@10
      value: 0.7837837837837838
      name: Cosine Recall@10
    - type: cosine_ndcg@10
      value: 0.6363124145429283
      name: Cosine Ndcg@10
    - type: cosine_mrr@10
      value: 0.5926748176748177
      name: Cosine Mrr@10
    - type: cosine_map@100
      value: 0.6079758940053057
      name: Cosine Map@100
---

# SentenceTransformer based on Snowflake/snowflake-arctic-embed-l

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [Snowflake/snowflake-arctic-embed-l](https://huggingface.co/Snowflake/snowflake-arctic-embed-l). It maps sentences & paragraphs to a 1024-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [Snowflake/snowflake-arctic-embed-l](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) <!-- at revision d8fb21ca8d905d2832ee8b96c894d3298964346b -->
- **Maximum Sequence Length:** 512 tokens
- **Output Dimensionality:** 1024 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ü§ó Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    "QUESTION #2\\n...\\n\\nContext:\\nNothing yet from Anthropic or Meta but I would be very surprised if they don‚Äôt have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\\n\\nWas the best currently available LLM trained in China for less than $6m?\\n\\nNot quite, but almost! It does make for a great attention-grabbing headline.\\n\\nThe big news to end the year was the release of DeepSeek v3‚Äîdropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\\n', additional_kwargs={}, response_metadata={})]",
    'Nothing yet from Anthropic or Meta but I would be very surprised if they don‚Äôt have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3‚Äîdropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.',
    "Industry‚Äôs Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\n\nPosted \n\n31st December 2023 at 11:59 pm ¬∑ Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nLLM 0.22, the annotated release notes - 17th February 2025\n\nRun LLMs on macOS using llm-mlx and Apple's MLX framework - 15th February 2025\n\nURL-addressable Pyodide Python environments - 13th February 2025\n\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\nblogging\n            68\n\nai\n            1100",
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 1024]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

## Evaluation

### Metrics

#### Information Retrieval

* Evaluated with [<code>InformationRetrievalEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.InformationRetrievalEvaluator)

| Metric              | Value      |
|:--------------------|:-----------|
| cosine_accuracy@1   | 0.5405     |
| cosine_accuracy@3   | 0.5946     |
| cosine_accuracy@5   | 0.6486     |
| cosine_accuracy@10  | 0.7838     |
| cosine_precision@1  | 0.5405     |
| cosine_precision@3  | 0.1982     |
| cosine_precision@5  | 0.1297     |
| cosine_precision@10 | 0.0784     |
| cosine_recall@1     | 0.5405     |
| cosine_recall@3     | 0.5946     |
| cosine_recall@5     | 0.6486     |
| cosine_recall@10    | 0.7838     |
| **cosine_ndcg@10**  | **0.6363** |
| cosine_mrr@10       | 0.5927     |
| cosine_map@100      | 0.608      |

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 152 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 152 samples:
  |         | sentence_0                                                                        | sentence_1                                                                           |
  |:--------|:----------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|
  | type    | string                                                                            | string                                                                               |
  | details | <ul><li>min: 4 tokens</li><li>mean: 74.0 tokens</li><li>max: 263 tokens</li></ul> | <ul><li>min: 66 tokens</li><li>mean: 143.86 tokens</li><li>max: 212 tokens</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | sentence_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
  |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>QUESTION #1\n</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | <code>Simon Willison‚Äôs Weblog<br><br>Subscribe<br><br>Stuff we figured out about AI in 2023<br><br>31st December 2023<br><br>2023 was the breakthrough year for Large Language Models (LLMs). I think it‚Äôs OK to call these AI‚Äîthey‚Äôre the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.<br><br>Here‚Äôs my attempt to round up the highlights in one place!<br><br>Large Language Models<br><br>They‚Äôre actually quite easy to build<br><br>You can run LLMs on your own devices<br><br>Hobbyists can build their own fine-tuned models<br><br>We don‚Äôt yet know how to build GPT-4<br><br>Vibes Based Development<br><br>LLMs are really smart, and also really, really dumb<br><br>Gullibility is the biggest unsolved problem<br><br>Code may be the best application</code> |
  | <code>QUESTION #2\n...\n\nContext:\nSimon Willison‚Äôs Weblog\n\nSubscribe\n\nStuff we figured out about AI in 2023\n\n31st December 2023\n\n2023 was the breakthrough year for Large Language Models (LLMs). I think it‚Äôs OK to call these AI‚Äîthey‚Äôre the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\n\nHere‚Äôs my attempt to round up the highlights in one place!\n\nLarge Language Models\n\nThey‚Äôre actually quite easy to build\n\nYou can run LLMs on your own devices\n\nHobbyists can build their own fine-tuned models\n\nWe don‚Äôt yet know how to build GPT-4\n\nVibes Based Development\n\nLLMs are really smart, and also really, really dumb\n\nGullibility is the biggest unsolved problem\n\nCode may be the best application\n', additional_kwargs={}, response_metadata={})]</code> | <code>Simon Willison‚Äôs Weblog<br><br>Subscribe<br><br>Stuff we figured out about AI in 2023<br><br>31st December 2023<br><br>2023 was the breakthrough year for Large Language Models (LLMs). I think it‚Äôs OK to call these AI‚Äîthey‚Äôre the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.<br><br>Here‚Äôs my attempt to round up the highlights in one place!<br><br>Large Language Models<br><br>They‚Äôre actually quite easy to build<br><br>You can run LLMs on your own devices<br><br>Hobbyists can build their own fine-tuned models<br><br>We don‚Äôt yet know how to build GPT-4<br><br>Vibes Based Development<br><br>LLMs are really smart, and also really, really dumb<br><br>Gullibility is the biggest unsolved problem<br><br>Code may be the best application</code> |
  | <code>QUESTION #1\n</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | <code>The ethics of this space remain diabolically complex<br><br>My blog in 2023<br><br>Here‚Äôs the sequel to this post: Things we learned about LLMs in 2024.<br><br>Large Language Models<br><br>In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.<br><br>LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.<br><br>They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.</code>                                                                                                                     |
* Loss: [<code>MatryoshkaLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#matryoshkaloss) with these parameters:
  ```json
  {
      "loss": "MultipleNegativesRankingLoss",
      "matryoshka_dims": [
          768,
          512,
          256,
          128,
          64
      ],
      "matryoshka_weights": [
          1,
          1,
          1,
          1,
          1
      ],
      "n_dims_per_step": -1
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `eval_strategy`: steps
- `per_device_train_batch_size`: 10
- `per_device_eval_batch_size`: 10
- `num_train_epochs`: 10
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: steps
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 10
- `per_device_eval_batch_size`: 10
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 10
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `dispatch_batches`: None
- `split_batches`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Training Logs
| Epoch | Step | cosine_ndcg@10 |
|:-----:|:----:|:--------------:|
| 1.0   | 16   | 0.6263         |
| 2.0   | 32   | 0.6363         |


### Framework Versions
- Python: 3.13.1
- Sentence Transformers: 3.4.1
- Transformers: 4.49.0
- PyTorch: 2.6.0+cpu
- Accelerate: 0.26.0
- Datasets: 3.3.1
- Tokenizers: 0.21.0

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MatryoshkaLoss
```bibtex
@misc{kusupati2024matryoshka,
    title={Matryoshka Representation Learning},
    author={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},
    year={2024},
    eprint={2205.13147},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->