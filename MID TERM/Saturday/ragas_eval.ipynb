{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_queries() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Prepare a diverse set of test queries covering different travel scenarios\n",
    "    Returns a list of dictionaries containing test cases\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"query\": \"Plan a 5-day trip to Tokyo in March\",\n",
    "            \"expected_agent\": \"itinerary_agent\",\n",
    "            \"category\": \"itinerary\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What are the best flight options from NYC to London next month?\",\n",
    "            \"expected_agent\": \"flight_agent\",\n",
    "            \"category\": \"flights\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Recommend hotels in Paris near the Eiffel Tower\",\n",
    "            \"expected_agent\": \"accommodation_agent\",\n",
    "            \"category\": \"accommodation\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What's the best time to visit Rome?\",\n",
    "            \"expected_agent\": \"information_agent\",\n",
    "            \"category\": \"information\"\n",
    "        },\n",
    "        # Add more test cases covering edge cases and complex scenarios\n",
    "        {\n",
    "            \"query\": \"Plan a budget-friendly European tour covering 3 countries in 10 days\",\n",
    "            \"expected_agent\": \"itinerary_agent\",\n",
    "            \"category\": \"complex_itinerary\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_response(travel_assistant, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get response from the travel assistant for a given query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = travel_assistant.invoke({\"query\": query})\n",
    "        return {\n",
    "            \"response\": result.get(\"agent_response\", \"\"),\n",
    "            \"agent_used\": result.get(\"agent_executor\", \"\"),\n",
    "            \"context\": result.get(\"context\", {})\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting response for query: {query}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        return {\"response\": \"\", \"agent_used\": \"\", \"context\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ragas_dataset(test_results: List[Dict[str, Any]]) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert test results into a format suitable for RAGAS evaluation\n",
    "    \"\"\"\n",
    "    dataset_dict = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "        \"contexts\": [],\n",
    "        \"reference\": []  # Added reference column for ground truth\n",
    "    }\n",
    "    \n",
    "    for result in test_results:\n",
    "        dataset_dict[\"question\"].append(result[\"query\"])\n",
    "        dataset_dict[\"answer\"].append(result[\"response\"])\n",
    "        # Convert context to list format as required by RAGAS\n",
    "        contexts = [str(value) for value in result[\"context\"].values()]\n",
    "        dataset_dict[\"contexts\"].append(contexts)\n",
    "        # Add the reference (ground truth) answer\n",
    "        dataset_dict[\"reference\"].append(result[\"reference\"])\n",
    "    \n",
    "    return Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(travel_assistant) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the multi-agent pipeline using RAGAS metrics\n",
    "    \"\"\"\n",
    "    # Prepare test queries\n",
    "    test_cases = prepare_test_queries()\n",
    "    \n",
    "    # Collect responses for all test cases\n",
    "    test_results = []\n",
    "    for case in test_cases:\n",
    "        response_data = get_agent_response(travel_assistant, case[\"query\"])\n",
    "        test_results.append({\n",
    "            \"query\": case[\"query\"],\n",
    "            \"expected_agent\": case[\"expected_agent\"],\n",
    "            \"category\": case[\"category\"],\n",
    "            \"reference\": case[\"reference\"],\n",
    "            **response_data\n",
    "        })\n",
    "    \n",
    "    # Create RAGAS dataset\n",
    "    evaluation_dataset = create_ragas_dataset(test_results)\n",
    "    \n",
    "    # Define evaluation metrics\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall\n",
    "    ]\n",
    "    \n",
    "    # Run RAGAS evaluation\n",
    "    try:\n",
    "        results = evaluate(evaluation_dataset, metrics)\n",
    "        \n",
    "        # Convert results to dictionary\n",
    "        metrics_dict = {\n",
    "            \"faithfulness\": float(results['faithfulness']),\n",
    "            \"answer_relevancy\": float(results['answer_relevancy']),\n",
    "            \"context_precision\": float(results['context_precision']),\n",
    "            \"context_recall\": float(results['context_recall'])\n",
    "        }\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        agent_accuracy = sum(1 for r in test_results \n",
    "                           if r[\"agent_used\"] == r[\"expected_agent\"]) / len(test_results)\n",
    "        metrics_dict[\"agent_routing_accuracy\"] = agent_accuracy\n",
    "        \n",
    "        return metrics_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during RAGAS evaluation: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results_by_category(test_results: List[Dict[str, Any]], \n",
    "                              metrics: Dict[str, float]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Analyze performance metrics by query category\n",
    "    \"\"\"\n",
    "    categories = {}\n",
    "    for result in test_results:\n",
    "        category = result[\"category\"]\n",
    "        if category not in categories:\n",
    "            categories[category] = {\n",
    "                \"count\": 0,\n",
    "                \"routing_accuracy\": 0,\n",
    "                \"response_quality\": []\n",
    "            }\n",
    "        categories[category][\"count\"] += 1\n",
    "        categories[category][\"routing_accuracy\"] += (\n",
    "            1 if result[\"agent_used\"] == result[\"expected_agent\"] else 0\n",
    "        )\n",
    "    \n",
    "    # Calculate averages\n",
    "    for category in categories:\n",
    "        categories[category][\"routing_accuracy\"] /= categories[category][\"count\"]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detailed_results(test_results: List[Dict[str, Any]], metrics: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    Save detailed evaluation results to a file\n",
    "    \"\"\"\n",
    "    output = {\n",
    "        \"overall_metrics\": metrics,\n",
    "        \"detailed_results\": []\n",
    "    }\n",
    "    \n",
    "    for result in test_results:\n",
    "        output[\"detailed_results\"].append({\n",
    "            \"query\": result[\"query\"],\n",
    "            \"expected_agent\": result[\"expected_agent\"],\n",
    "            \"actual_agent\": result[\"agent_used\"],\n",
    "            \"category\": result[\"category\"],\n",
    "            \"response\": result[\"response\"][:500] + \"...\" if len(result[\"response\"]) > 500 else result[\"response\"]\n",
    "        })\n",
    "    \n",
    "    with open(\"detailed_evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in evaluation: name 'create_travel_assistant_graph' is not defined\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the evaluation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create travel assistant instance\n",
    "        travel_assistant = create_travel_assistant_graph()\n",
    "        \n",
    "        # Run evaluation\n",
    "        metrics = evaluate_pipeline(travel_assistant)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nRAGAS Evaluation Results:\")\n",
    "        print(\"-\" * 30)\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.3f}\")\n",
    "        \n",
    "        # Get test results for detailed analysis\n",
    "        test_results = [get_agent_response(travel_assistant, case[\"query\"]) \n",
    "                       for case in prepare_test_queries()]\n",
    "        \n",
    "        # Analyze by category\n",
    "        category_analysis = analyze_results_by_category(test_results, metrics)\n",
    "        \n",
    "        print(\"\\nPerformance by Category:\")\n",
    "        print(\"-\" * 30)\n",
    "        for category, stats in category_analysis.items():\n",
    "            print(f\"{category}:\")\n",
    "            print(f\"  Routing Accuracy: {stats['routing_accuracy']:.3f}\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        save_detailed_results(test_results, metrics)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
