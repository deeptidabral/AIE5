{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping langchain as it is not installed.\n",
      "WARNING: Skipping langchain-core as it is not installed.\n",
      "WARNING: Skipping langchain-openai as it is not installed.\n",
      "WARNING: Skipping langchain-community as it is not installed.\n",
      "WARNING: Skipping langchain-text-splitters as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y langchain langchain-core langchain-openai langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting langchain-core\n",
      "  Using cached langchain_core-0.3.39-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Using cached langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.2.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain-openai) (1.64.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (2.7.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\dabra\\appdata\\roaming\\python\\python313\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Using cached langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.39-py3-none-any.whl (414 kB)\n",
      "Using cached langchain_openai-0.3.7-py3-none-any.whl (55 kB)\n",
      "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 27.8 MB/s eta 0:00:00\n",
      "Using cached langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "Successfully installed langchain-0.3.19 langchain-community-0.3.18 langchain-core-0.3.39 langchain-openai-0.3.7 langchain-text-splitters-0.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.19\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-community, ragas\n",
      "---\n",
      "Name: langchain-core\n",
      "Version: 0.3.39\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity, typing-extensions\n",
      "Required-by: langchain, langchain-cohere, langchain-community, langchain-huggingface, langchain-openai, langchain-qdrant, langchain-text-splitters, langgraph, langgraph-checkpoint, ragas\n",
      "---\n",
      "Name: langchain-openai\n",
      "Version: 0.3.7\n",
      "Summary: An integration package connecting OpenAI and LangChain\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: langchain-core, openai, tiktoken\n",
      "Required-by: ragas\n",
      "---\n",
      "Name: langchain-community\n",
      "Version: 0.3.18\n",
      "Summary: Community contributed LangChain integrations.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: aiohttp, dataclasses-json, httpx-sse, langchain, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-cohere, ragas\n",
      "---\n",
      "Name: langchain-text-splitters\n",
      "Version: 0.3.6\n",
      "Summary: LangChain text splitting utilities\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\n",
      "Requires: langchain-core\n",
      "Required-by: langchain\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain langchain-core langchain-openai langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"XX\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "# Set the LLM Configuration\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"agent_state.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"multi_agent_definition.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_travel_assistant(filename='travel_assistant.pkl'):\n",
    "    try:\n",
    "        filepath = os.path.join('saved_models', filename)\n",
    "        \n",
    "        # Load using dill\n",
    "        with open(filepath, 'rb') as f:\n",
    "            saved_data = dill.load(f)\n",
    "        \n",
    "        # Recreate the graph using the saved creation function\n",
    "        travel_assistant = saved_data['create_graph_function']()\n",
    "        \n",
    "        print(f\"Travel assistant loaded from {filepath}\")\n",
    "        return travel_assistant\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading travel assistant: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the agent\n",
    "travel_assistant = load_travel_assistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_test_set(travel_db, num_tests=30):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive test set targeting different agents and scenarios\n",
    "    \"\"\"\n",
    "    # Initialize LLM for test generation\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "    \n",
    "    # Test scenario categories\n",
    "    test_categories = [\n",
    "        # Itinerary Agent Tests\n",
    "        {\n",
    "            \"agent\": \"Itinerary\",\n",
    "            \"scenarios\": [\n",
    "                \"Create a 7-day itinerary for a solo traveler\",\n",
    "                \"Plan a family vacation with children\",\n",
    "                \"Design a romantic honeymoon trip\",\n",
    "                \"Develop an adventure-focused travel plan\",\n",
    "                \"Create a budget-friendly multi-city trip\",\n",
    "                \"Plan a cultural immersion journey\",\n",
    "                \"Design an eco-tourism adventure\"\n",
    "            ]\n",
    "        },\n",
    "        # Flight Agent Tests\n",
    "        {\n",
    "            \"agent\": \"Flight\",\n",
    "            \"scenarios\": [\n",
    "                \"Find the most cost-effective flight options\",\n",
    "                \"Compare flight routes with layovers\",\n",
    "                \"Identify flights with best travel times\",\n",
    "                \"Find flights with minimal connections\",\n",
    "                \"Compare business class options\",\n",
    "                \"Find flights with best baggage allowance\",\n",
    "                \"Locate flights with most airline rewards\"\n",
    "            ]\n",
    "        },\n",
    "        # Information Agent Tests\n",
    "        {\n",
    "            \"agent\": \"Information\",\n",
    "            \"scenarios\": [\n",
    "                \"Provide detailed travel visa requirements\",\n",
    "                \"Give comprehensive destination safety information\",\n",
    "                \"Explain local cultural customs and etiquette\",\n",
    "                \"Provide detailed weather information\",\n",
    "                \"Recommend best travel seasons\",\n",
    "                \"Give insights on local transportation\",\n",
    "                \"Provide health and vaccination guidance\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Test set storage\n",
    "    test_set = {\n",
    "        \"question\": [],\n",
    "        \"agent_target\": [],\n",
    "        \"context\": [],\n",
    "        \"reference_answer\": []\n",
    "    }\n",
    "    \n",
    "    # Extract contexts from travel database\n",
    "    contexts = []\n",
    "    for doc_id in travel_db.index_to_docstore_id.values():\n",
    "        doc = travel_db.docstore.search(doc_id)\n",
    "        if len(doc.page_content) >= 1000:\n",
    "            contexts.append(doc.page_content)\n",
    "    \n",
    "    # Ensure we have enough contexts\n",
    "    if len(contexts) < num_tests:\n",
    "        raise ValueError(\"Not enough contexts in the travel database\")\n",
    "    \n",
    "    # Randomly sample contexts\n",
    "    sampled_contexts = random.sample(contexts, num_tests)\n",
    "    \n",
    "    # Generate test cases\n",
    "    for i in range(num_tests):\n",
    "        # Select a category and scenario\n",
    "        category = random.choice(test_categories)\n",
    "        scenario = random.choice(category[\"scenarios\"])\n",
    "        context = sampled_contexts[i]\n",
    "        \n",
    "        # Generate context-aware question\n",
    "        question_prompt = f\"\"\"\n",
    "        Based on the following travel context, create a {category['agent']} agent-specific question that tests:\n",
    "        - Depth of knowledge\n",
    "        - Ability to provide nuanced recommendations\n",
    "        - Understanding of travel complexities\n",
    "\n",
    "        Context Snippet: {context[:1000]}\n",
    "        Scenario: {scenario}\n",
    "\n",
    "        Generate a specific, challenging question that requires sophisticated reasoning.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate question\n",
    "        question_response = llm.invoke(question_prompt)\n",
    "        question = question_response.content\n",
    "        \n",
    "        # Generate reference answer\n",
    "        answer_prompt = f\"\"\"\n",
    "        Provide a comprehensive, detailed answer to the following question \n",
    "        using the given context. The answer should demonstrate:\n",
    "        - Deep understanding of the travel scenario\n",
    "        - Specific, actionable recommendations\n",
    "        - Nuanced insights\n",
    "\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        answer_response = llm.invoke(answer_prompt)\n",
    "        reference_answer = answer_response.content\n",
    "        \n",
    "        # Store test case\n",
    "        test_set[\"question\"].append(question)\n",
    "        test_set[\"agent_target\"].append(category[\"agent\"])\n",
    "        test_set[\"context\"].append(context)\n",
    "        test_set[\"reference_answer\"].append(reference_answer)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(test_set)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(\"comprehensive_travel_agent_test_set.csv\", index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_test_set(test_set_df):\n",
    "    \"\"\"\n",
    "    Validate the generated test set\n",
    "    \"\"\"\n",
    "    # Check number of tests\n",
    "    assert len(test_set_df) == 30, \"Test set should have exactly 30 tests\"\n",
    "    \n",
    "    # Check agent distribution\n",
    "    agent_distribution = test_set_df['agent_target'].value_counts()\n",
    "    print(\"Agent Distribution:\")\n",
    "    print(agent_distribution)\n",
    "    \n",
    "    # Ensure balanced representation of agents\n",
    "    assert len(agent_distribution) == 3, \"Should have tests for all three agents\"\n",
    "    \n",
    "    # Check question and answer lengths\n",
    "    print(\"\\nQuestion Length Statistics:\")\n",
    "    print(test_set_df['question'].str.len().describe())\n",
    "    \n",
    "    print(\"\\nReference Answer Length Statistics:\")\n",
    "    print(test_set_df['reference_answer'].str.len().describe())\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "def main(travel_db):\n",
    "    # Generate test set\n",
    "    test_set_df = generate_comprehensive_test_set(travel_db)\n",
    "    \n",
    "    # Validate test set\n",
    "    validate_test_set(test_set_df)\n",
    "    \n",
    "    print(\"\\nComprehensive test set generated successfully!\")\n",
    "    return test_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Ensure to pass your travel_db when calling this function\n",
    "test_set = main(travel_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"comprehensive_travel_agent_test_set.csv\")\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses using the travel assistant\n",
    "generated_answers = []\n",
    "    \n",
    "print(\"Generating responses for evaluation...\")\n",
    "for _, row in test_set.iterrows():\n",
    "    try:\n",
    "        # Invoke the travel assistant with the question\n",
    "        response = travel_assistant.invoke({\n",
    "            \"query\": row['question']\n",
    "        })\n",
    "            \n",
    "        # Convert response to string if it's not already\n",
    "        generated_answer = str(response)\n",
    "        generated_answers.append(generated_answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response for question: {row['question']}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        generated_answers.append(\"Error generating response\")\n",
    "    \n",
    "# Add generated answers to the DataFrame\n",
    "test_set['generated_answer'] = generated_answers\n",
    "\n",
    "test_set.to_csv(\"test_set_updated_with_agent_answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for RAGAS evaluation\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    'question': test_set['question'],\n",
    "    'answer': test_set['generated_answer'],                 # Use the generated answers\n",
    "    'ground_truth': test_set['reference_answer'],           # Reference answers for comparison\n",
    "    'contexts': test_set['context'].apply(lambda x: [x]),   # Wrap context in a list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAGAS evaluation\n",
    "print(\"Running RAGAS evaluation...\")\n",
    "results = evaluate(eval_dataset, metrics=[faithfulness, answer_relevancy, context_precision, context_recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and analyze results\n",
    "print(\"\\nRAGAS Evaluation Results:\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown by agent type\n",
    "agent_results = {}\n",
    "\n",
    "for agent in test_set['agent_target'].unique():\n",
    "    # print(test_set['agent_target'])\n",
    "    agent_subset = eval_dataset.filter(lambda x, idx: test_set.iloc[idx]['agent_target'] == agent, with_indices=True)\n",
    "    agent_eval_results = evaluate(agent_subset, metrics=[faithfulness,answer_relevancy,context_precision,context_recall])\n",
    "    agent_results[agent] = agent_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and analyze results by agent type\n",
    "print(\"\\nRAGAS Evaluation Results:\")\n",
    "\n",
    "agent_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_df = pd.DataFrame({\n",
    "    'question': test_set['question'],\n",
    "    'agent_target': test_set['agent_target'],\n",
    "    'generated_answer': test_set['generated_answer'],\n",
    "    'reference_answer': test_set['reference_answer'],\n",
    "    'faithfulness': results['faithfulness'],\n",
    "    'answer_relevancy': results['answer_relevancy'],\n",
    "    'context_precision': results['context_precision'],\n",
    "    'context_recall': results['context_recall']\n",
    "})\n",
    "\n",
    "results_df.to_csv('ragas_evaluation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_ragas_evaluation(travel_assistant, test_set_path='comprehensive_travel_agent_test_set.csv'):\n",
    "    # Read the test set\n",
    "    df = pd.read_csv(test_set_path)\n",
    "\n",
    "    # df.rename(columns={'reference_answer': 'reference'}, inplace=True)\n",
    "    \n",
    "    # Generate responses using the travel assistant\n",
    "    generated_answers = []\n",
    "    \n",
    "    print(\"Generating responses for evaluation...\")\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            # Invoke the travel assistant with the question\n",
    "            response = travel_assistant.invoke({\n",
    "                \"query\": row['question']\n",
    "            })\n",
    "            \n",
    "            # Convert response to string if it's not already\n",
    "            generated_answer = str(response)\n",
    "            generated_answers.append(generated_answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response for question: {row['question']}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            generated_answers.append(\"Error generating response\")\n",
    "    \n",
    "    # Add generated answers to the DataFrame\n",
    "    df['generated_answer'] = generated_answers\n",
    "    \n",
    "    # Prepare dataset for RAGAS evaluation\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        'question': df['question'],\n",
    "        'answer': df['generated_answer'],  # Use the generated answers\n",
    "        'ground_truth': df['reference_answer'],  # Reference answers for comparison\n",
    "        'contexts': df['context'].apply(lambda x: [x]),  # Wrap context in a list\n",
    "    })\n",
    "    \n",
    "    # Run RAGAS evaluation\n",
    "    print(\"Running RAGAS evaluation...\")\n",
    "    results = evaluate(\n",
    "        eval_dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Print and analyze results\n",
    "    print(\"\\nRAGAS Evaluation Results:\")\n",
    "    for metric, score in results.items():\n",
    "        print(f\"{metric}: {score}\")\n",
    "    \n",
    "    # Detailed breakdown by agent type\n",
    "    agent_results = {}\n",
    "    for agent in df['agent_target'].unique():\n",
    "        agent_subset = eval_dataset.filter(\n",
    "            lambda x, idx: df.loc[idx, 'agent_target'] == agent\n",
    "        )\n",
    "        \n",
    "        agent_eval_results = evaluate(\n",
    "            agent_subset, \n",
    "            metrics=[\n",
    "                faithfulness,\n",
    "                answer_relevancy,\n",
    "                context_precision,\n",
    "                context_recall\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        agent_results[agent] = agent_eval_results\n",
    "    \n",
    "    # Print agent-specific results\n",
    "    print(\"\\nAgent-Specific Evaluation:\")\n",
    "    for agent, agent_metrics in agent_results.items():\n",
    "        print(f\"\\n{agent} Agent:\")\n",
    "        for metric, score in agent_metrics.items():\n",
    "            print(f\"  {metric}: {score}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = pd.DataFrame({\n",
    "        'question': df['question'],\n",
    "        'agent_target': df['agent_target'],\n",
    "        'generated_answer': df['generated_answer'],\n",
    "        'reference_answer': df['reference_answer'],\n",
    "        'faithfulness': results['faithfulness'],\n",
    "        'answer_relevancy': results['answer_relevancy'],\n",
    "        'context_precision': results['context_precision'],\n",
    "        'context_recall': results['context_recall']\n",
    "    })\n",
    "    \n",
    "    results_df.to_csv('ragas_evaluation_results.csv', index=False)\n",
    "    \n",
    "    return {\n",
    "        'overall_results': results,\n",
    "        'agent_results': agent_results,\n",
    "        'results_dataframe': results_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "evaluation_results = run_comprehensive_ragas_evaluation(travel_assistant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
