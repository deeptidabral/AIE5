{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 03:32:41 - PyTorch version 2.6.0 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n",
      "C:\\Users\\dabra\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\v1\\typing.py:68: DeprecationWarning: Failing to pass a value to the 'type_params' parameter of 'typing.ForwardRef._evaluate' is deprecated, as it leads to incorrect behaviour when calling typing.ForwardRef._evaluate on a stringified annotation that references a PEP 695 type parameter. It will be disallowed in Python 3.15.\n",
      "  return cast(Any, type_)._evaluate(globalns, localns, recursive_guard=set())\n"
     ]
    }
   ],
   "source": [
    "%run \"00_wall_of_imports.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dabra\\AppData\\Local\\Temp\\ipykernel_21660\\1919260345.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key:\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.7)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector store\n",
    "travel_db = FAISS.load_local(\"travel_db_faiss\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Load the docstore\n",
    "with open('travel_db_docstore.pkl', 'rb') as f:\n",
    "    travel_db.docstore = pickle.load(f)\n",
    "\n",
    "# Verify it loaded correctly\n",
    "print(f\"Loaded knowledge base with {len(travel_db.index_to_docstore_id)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AgentState class as in the notebook\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"State for the travel assistant workflow.\"\"\"\n",
    "\n",
    "    query: str\n",
    "    chat_history: List[Any] = Field(default_factory=list)\n",
    "    agent_executor: Optional[str] = None\n",
    "    agent_response: Optional[str] = None\n",
    "    final_response: Optional[str] = None\n",
    "    context: Dict[str, Any] = Field(default_factory=dict)\n",
    "    error: Optional[str] = None\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "    \n",
    "    def __repr__(self):\n",
    "         # Defines string representation of the AgentState object\n",
    "        return f\"AgentState(query={self.query}, agent_executor={self.agent_executor})\"\n",
    "        \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert state to dictionary for LangGraph compatibility\"\"\"\n",
    "        # Method to convert the state object to a dictionary\n",
    "        return {\n",
    "            \"query\": self.query,\n",
    "            \"chat_history\": self.chat_history,\n",
    "            \"agent_executor\": self.agent_executor,\n",
    "            \"agent_response\": self.agent_response,\n",
    "            \"final_response\": self.final_response,\n",
    "            \"context\": self.context,\n",
    "            \"error\": self.error,\n",
    "            \"messages\": self.messages\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API key\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.7)\n",
    "    \n",
    "    # Load the vector store\n",
    "    try:\n",
    "        travel_db = FAISS.load_local(\"travel_db_faiss\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "        \n",
    "        # Load the docstore\n",
    "        with open('travel_db_docstore.pkl', 'rb') as f:\n",
    "            travel_db.docstore = pickle.load(f)\n",
    "            \n",
    "        print(f\"Loaded knowledge base with {len(travel_db.index_to_docstore_id)} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading knowledge base: {e}\")\n",
    "        travel_db = None\n",
    "\n",
    "    # Define the router agent function\n",
    "    def router_agent(state: AgentState) -> dict:\n",
    "        \"\"\"Router agent that determines which specialized agent should handle the query.\"\"\"\n",
    "\n",
    "        # Create a chat prompt template with system and human messages\n",
    "        router_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a travel assistant router. Your job is to determine which specialized agent\n",
    "            should handle the user's travel-related query. Choose the most appropriate agent from:\n",
    "            \n",
    "            - itinerary_agent: For requests to create travel itineraries, vacation plans, or multi-day travel schedules\n",
    "            - flight_agent: For questions about flights, airfares, airlines, or flight bookings\n",
    "            - accommodation_agent: For questions about hotels, resorts, accommodations, or places to stay\n",
    "            - information_agent: For general travel information, destination facts, or travel advice\n",
    "            \n",
    "            Respond ONLY with the name of the appropriate agent. Do not include any explanations or additional text.\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{query}\")\n",
    "        ])\n",
    "        \n",
    "        # Use a chain to pass the user query to the LLM, get the recommended agent name and parse the output as a string\n",
    "        chain = router_prompt | llm | (lambda x: x.content)\n",
    "\n",
    "        agent_executor = chain.invoke({\"query\": state.query}).strip()\n",
    "        \n",
    "        valid_agents = [\"itinerary_agent\", \"flight_agent\", \"accommodation_agent\", \"information_agent\"]\n",
    "\n",
    "        # Validate the response\n",
    "        if agent_executor not in valid_agents:\n",
    "            # Default to information agent if invalid response\n",
    "            agent_executor = \"information_agent\"\n",
    "                \n",
    "        # Return state as a dictionary\n",
    "        return {\"agent_executor\": agent_executor, \n",
    "                \"query\": state.query,\n",
    "                \"chat_history\": state.chat_history,\n",
    "                \"agent_response\": state.agent_response,\n",
    "                \"final_response\": state.final_response,\n",
    "                \"context\": state.context,\n",
    "                \"error\": state.error}\n",
    "\n",
    "    # Setup RAG chain for information retrieval\n",
    "    def setup_rag_chain():\n",
    "        \"\"\"Set up the RAG chain for information retrieval.\"\"\"\n",
    "        # Create a retriever from our travel knowledge base\n",
    "        if travel_db is not None:\n",
    "            retriever = travel_db.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": 7}\n",
    "            )\n",
    "                \n",
    "            rag_prompt = ChatPromptTemplate.from_template(\"\"\"You are a knowledgeable travel assistant with expertise in destinations worldwide.\n",
    "                Use the following travel information to provide detailed, accurate responses to the user's query.\n",
    "                If the retrieved information doesn't fully answer the question, use your knowledge to provide\n",
    "                the best possible response, but prioritize the retrieved information.\n",
    "                \n",
    "                Retrieved information: {context}        \n",
    "                Question: {input}\n",
    "            \"\"\")\n",
    "\n",
    "            # Create the document processing chain\n",
    "            document_chain = create_stuff_documents_chain(llm, rag_prompt)\n",
    "            \n",
    "            return create_retrieval_chain(retriever, document_chain)\n",
    "        else:\n",
    "            # Return None if vector store is not available\n",
    "            return None\n",
    "\n",
    "    # Define specialized agents\n",
    "    def itinerary_agent(state: AgentState) -> dict:\n",
    "        \"\"\"Creates customized travel itineraries based on user preferences.\"\"\"\n",
    "        try:\n",
    "            # First, retrieve relevant destination information\n",
    "            rag_chain = setup_rag_chain()\n",
    "            if rag_chain:\n",
    "                retrieval_result = rag_chain.invoke({\"input\": state.query})\n",
    "            else:\n",
    "                retrieval_result = {\"answer\": \"I don't have specific information about this destination, but I'll do my best to help.\"}\n",
    "            \n",
    "            # Extract destination information from query using LLM\n",
    "            extraction_prompt = ChatPromptTemplate.from_template(\"\"\"Extract the key travel information from the user's query.\n",
    "            Return a JSON object with these fields (leave empty if not mentioned):\n",
    "            {{\n",
    "                \"destinations\": [\"list of mentioned destinations\"],\n",
    "                \"duration\": \"total trip duration in days\",\n",
    "                \"budget\": \"budget information\",\n",
    "                \"interests\": [\"list of mentioned interests/activities\"],\n",
    "                \"travel_dates\": \"approximate travel dates\",\n",
    "                \"travelers\": \"number and type of travelers (family, couple, solo, etc.)\"\n",
    "            }}\n",
    "            \n",
    "            Query: {input}\n",
    "            \"\"\")       \n",
    "            \n",
    "            extraction_chain = extraction_prompt | llm | (lambda x: x.content)\n",
    "\n",
    "            try:\n",
    "                # First check if the string is not empty\n",
    "                extraction_result = extraction_chain.invoke({\"input\": state.query}).strip()\n",
    "                if extraction_result:\n",
    "                    extracted_info = json.loads(extraction_result)\n",
    "                else:\n",
    "                    extracted_info = {\n",
    "                        \"destinations\": [],\n",
    "                        \"duration\": \"\",\n",
    "                        \"budget\": \"\",\n",
    "                        \"interests\": [],\n",
    "                        \"travel_dates\": \"\",\n",
    "                        \"travelers\": \"\"\n",
    "                    }\n",
    "                state.context.update(extracted_info)\n",
    "            except json.JSONDecodeError:\n",
    "                # Provide default structure when parsing fails\n",
    "                extracted_info = {\n",
    "                    \"destinations\": [],\n",
    "                    \"duration\": \"\",\n",
    "                    \"budget\": \"\",\n",
    "                    \"interests\": [],\n",
    "                    \"travel_dates\": \"\",\n",
    "                    \"travelers\": \"\"\n",
    "                }\n",
    "                state.context.update(extracted_info)\n",
    "\n",
    "            \n",
    "            # Generate itinerary using retrieved information and extracted parameters\n",
    "            itinerary_prompt = ChatPromptTemplate.from_template(\"\"\"You are a travel itinerary expert. Create a detailed day-by-day travel itinerary\n",
    "            based on the user's preferences and the retrieved destination information.\n",
    "            \n",
    "            For each day, include morning activities, lunch suggestions, afternoon activities, dinner recommendations, and \n",
    "            evening activities or relaxation options\n",
    "            \n",
    "            Also include practical advice about transportation between attractions, estimated costs, time management tips, and local customs.\n",
    "            \n",
    "            Make the itinerary realistic in terms of travel times and activities per day.\n",
    "            \n",
    "            Context information:\n",
    "            {context_str}\n",
    "            \n",
    "            Extracted travel parameters:\n",
    "            {parameters}\n",
    "            \n",
    "            Query: {input}\n",
    "            \"\"\")        \n",
    "            \n",
    "            # Format the context and parameters for the prompt\n",
    "            context_str = retrieval_result.get(\"answer\", \"\")\n",
    "            parameters_str = json.dumps(state.context, indent=2)\n",
    "            \n",
    "            itinerary_chain = itinerary_prompt | llm | (lambda x: x.content)\n",
    "            response = itinerary_chain.invoke({\n",
    "                \"input\": state.query,\n",
    "                \"context_str\": context_str,\n",
    "                \"parameters\": parameters_str\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                \"agent_response\": response,\n",
    "                \"context\": state.context,\n",
    "                \"query\": state.query\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"context\": state.context,\n",
    "                \"query\": state.query\n",
    "            }\n",
    "\n",
    "    def flight_agent(state: AgentState) -> dict:\n",
    "        \"\"\"Handles flight-related questions and searches.\"\"\"\n",
    "        try:\n",
    "            # Extract flight search parameters\n",
    "            extraction_prompt = ChatPromptTemplate.from_template(\"\"\"Extract flight search parameters from the user's query.\n",
    "                Return a JSON object with these fields (leave empty if not mentioned):\n",
    "            {{\n",
    "                \"origin\": \"origin airport or city code\",\n",
    "                \"destination\": \"destination airport or city code\",\n",
    "                \"departure_date\": \"departure date in YYYY-MM-DD format\",\n",
    "                \"return_date\": \"return date in YYYY-MM-DD format (if round-trip)\",\n",
    "                \"num_passengers\": \"number of passengers\",\n",
    "                \"cabin_class\": \"economy/business/first\",\n",
    "                \"price_range\": \"budget constraints\",\n",
    "                \"airline_preferences\": [\"preferred airlines\"]\n",
    "            }}\n",
    "            \n",
    "            Query: {input}\n",
    "            \"\"\")       \n",
    "            \n",
    "            extraction_chain = extraction_prompt | llm | (lambda x: x.content)\n",
    "\n",
    "            try:\n",
    "                flight_params = json.loads(extraction_chain.invoke({\"input\": state.query}))\n",
    "                state.context.update({\"flight_params\": flight_params})\n",
    "            except json.JSONDecodeError:\n",
    "                state.context.update({\"flight_params\": {}})\n",
    "            \n",
    "            # Get flight information using RAG\n",
    "            rag_chain = setup_rag_chain()\n",
    "            if rag_chain:\n",
    "                retrieval_result = rag_chain.invoke({\"input\": state.query})\n",
    "            else:\n",
    "                retrieval_result = {\"answer\": \"I don't have specific flight information in my database, but I can provide general advice.\"}\n",
    "\n",
    "            flight_prompt = ChatPromptTemplate.from_template(\"\"\"You are a flight search specialist. Provide helpful information about flights\n",
    "                based on the retrieved flight data and the user's query. Include details about available flights matching the criteria, \n",
    "                price ranges and fare comparisons, airline options, departure/arrival times, travel duration, layovers (if applicable), \n",
    "                and booking recommendations.\n",
    "                \n",
    "                If exact flight information isn't available in the retrieved data, provide general advice\n",
    "                about the requested route, typical prices, and best booking strategies.\n",
    "                \n",
    "                Retrieved flight information:\n",
    "                {context_str}\n",
    "                \n",
    "                Extracted flight parameters:\n",
    "                {parameters}\n",
    "            \n",
    "                Query: {input}\n",
    "                \"\"\")\n",
    "            \n",
    "            # Format the context and parameters for the prompt\n",
    "            context_str = retrieval_result.get(\"answer\", \"\")\n",
    "            parameters_str = json.dumps(state.context.get(\"flight_params\", {}), indent=2)\n",
    "            \n",
    "            flight_chain = flight_prompt | llm | (lambda x: x.content)\n",
    "            response = flight_chain.invoke({\n",
    "                \"input\": state.query,\n",
    "                \"context_str\": context_str,\n",
    "                \"parameters\": parameters_str\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                \"agent_response\": response,\n",
    "                \"context\": state.context,\n",
    "                \"query\": state.query\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"context\": state.context,\n",
    "                \"query\": state.query\n",
    "            }\n",
    "\n",
    "    def accommodation_agent(state: AgentState) -> dict:\n",
    "        \"\"\"Provides hotel and accommodation recommendations.\"\"\"\n",
    "        try:\n",
    "            # Extract accommodation preferences\n",
    "            extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"Extract accommodation preferences from the user's query.\n",
    "                Return a JSON object with these fields (leave empty if not mentioned):\n",
    "                {{\n",
    "                    \"location\": \"city or specific area\",\n",
    "                    \"check_in_date\": \"in YYYY-MM-DD format\",\n",
    "                    \"check_out_date\": \"in YYYY-MM-DD format\",\n",
    "                    \"guests\": \"number of guests\",\n",
    "                    \"rooms\": \"number of rooms\",\n",
    "                    \"budget_range\": \"price range per night\"\n",
    "                }}\n",
    "                \"\"\"),\n",
    "                (\"human\", \"{query}\")\n",
    "            ])\n",
    "            \n",
    "            extraction_chain = extraction_prompt | llm | (lambda x: x.content)\n",
    "            try:\n",
    "                accommodation_params = json.loads(extraction_chain.invoke({\"query\": state.query}))\n",
    "                state.context.update({\"accommodation_params\": accommodation_params})\n",
    "            except json.JSONDecodeError:\n",
    "                state.context.update({\"accommodation_params\": {}})\n",
    "            \n",
    "            # Get accommodation information using RAG\n",
    "            rag_chain = setup_rag_chain()\n",
    "            if rag_chain:\n",
    "                retrieval_result = rag_chain.invoke({\"input\": f\"hotels in {state.context.get('accommodation_params', {}).get('location', '')}\"})\n",
    "            else:\n",
    "                retrieval_result = {\"context\": \"I don't have specific accommodation information in my database, but I can provide general advice.\"}\n",
    "            \n",
    "            # Generate response based on accommodation parameters and retrieved information\n",
    "            accommodation_prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"You are a hotel and accommodation expert. Provide detailed recommendations\n",
    "                based on the user's preferences and the retrieved accommodation data. Include:\n",
    "                \n",
    "                - Suitable hotel/accommodation options\n",
    "                - Price ranges and value considerations\n",
    "                - Location benefits and proximity to attractions\n",
    "                - Amenities and facilities\n",
    "                - Guest ratings and reviews summary\n",
    "                - Booking tips and optimal timing\n",
    "                \n",
    "                If specific accommodation data isn't available, provide general advice about\n",
    "                accommodations in the requested location, typical options at different price points,\n",
    "                and best areas to stay.\n",
    "                \n",
    "                Retrieved accommodation information:\n",
    "                {context}\n",
    "                \n",
    "                Extracted accommodation parameters:\n",
    "                {accommodation_params}\n",
    "                \"\"\"),\n",
    "                (\"human\", \"{query}\")\n",
    "            ])\n",
    "            \n",
    "            accommodation_chain = accommodation_prompt | llm | (lambda x: x.content)\n",
    "            response = accommodation_chain.invoke({\n",
    "                \"query\": state.query,\n",
    "                \"context\": retrieval_result.get(\"context\", \"\"),\n",
    "                \"accommodation_params\": json.dumps(state.context.get(\"accommodation_params\", {}), indent=2)\n",
    "            })\n",
    "            \n",
    "            return {\"agent_response\": response}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def information_agent(state: AgentState) -> dict:\n",
    "        \"\"\"Answers general travel questions using RAG.\"\"\"\n",
    "        try:\n",
    "            # This agent directly uses the RAG chain to provide travel information\n",
    "            rag_chain = setup_rag_chain()\n",
    "            if rag_chain:\n",
    "                result = rag_chain.invoke({\"input\": state.query})\n",
    "            else:\n",
    "                result = {\"answer\": \"I don't have specific information about this in my travel database, but I'll provide general advice based on my knowledge.\"}\n",
    "            \n",
    "            # Enhance RAG response with additional context if needed\n",
    "            enhancement_prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"You are a knowledgeable travel information specialist. Review and enhance\n",
    "                the retrieved information to provide a comprehensive, accurate response to the user's query.\n",
    "                \n",
    "                If the retrieved information is incomplete, add relevant details from your knowledge while\n",
    "                clearly distinguishing between retrieved facts and general knowledge.\n",
    "                \n",
    "                Focus on providing practical, useful information that directly addresses the user's needs.\n",
    "                Include cultural insights, traveler tips, and seasonal considerations when relevant.\n",
    "                \n",
    "                Retrieved information:\n",
    "                {rag_response}\n",
    "                \"\"\"),\n",
    "                (\"human\", \"{input}\")\n",
    "            ])\n",
    "            \n",
    "            enhancement_chain = enhancement_prompt | llm | (lambda x: x.content)\n",
    "            enhanced_response = enhancement_chain.invoke({\n",
    "                \"input\": state.query,\n",
    "                \"rag_response\": result.get(\"answer\", \"\")\n",
    "            })\n",
    "            \n",
    "            return {\"agent_response\": enhanced_response}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    # Response Generator - Creates the final response\n",
    "    def generate_final_response(state: AgentState) -> dict:\n",
    "        \"\"\"Generates the final, polished response to the user.\"\"\"\n",
    "        # Create a consistent, helpful response format\n",
    "        formatting_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a friendly, helpful travel assistant. Format the specialized agent's response\n",
    "            into a clear, well-structured, and engaging reply. Maintain all the factual information and advice\n",
    "            while improving readability with:\n",
    "            \n",
    "            - A warm, conversational tone\n",
    "            - Logical organization with headings where appropriate\n",
    "            - Bullet points for lists\n",
    "            - Bold text for important information\n",
    "            - Emojis where appropriate (but not excessive)\n",
    "            \n",
    "            Make sure the response completely addresses the user's query. Add a brief, friendly closing\n",
    "            that invites further questions.\n",
    "            \n",
    "            Original agent response:\n",
    "            {agent_response}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{query}\")\n",
    "        ])\n",
    "        \n",
    "        formatting_chain = formatting_prompt | llm | (lambda x: x.content)\n",
    "        final_response = formatting_chain.invoke({\n",
    "            \"query\": state.query,\n",
    "            \"agent_response\": state.agent_response\n",
    "        })\n",
    "        \n",
    "        return {\"agent_response\": final_response}\n",
    "\n",
    "    # Error Handler - Manages errors gracefully\n",
    "    def handle_error(state: AgentState) -> dict:\n",
    "        \"\"\"Handles errors and provides a graceful fallback response.\"\"\"\n",
    "        error_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful travel assistant. The system encountered an error while\n",
    "            processing the user's query. Provide a helpful response that:\n",
    "            \n",
    "            1. Acknowledges the issue\n",
    "            2. Offers general travel advice related to their query\n",
    "            3. Suggests how they might rephrase their question for better results\n",
    "            \n",
    "            Error message: {error}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{query}\")\n",
    "        ])\n",
    "        \n",
    "        error_chain = error_prompt | llm | (lambda x: x.content)\n",
    "        fallback_response = error_chain.invoke({\n",
    "            \"query\": state.query,\n",
    "            \"error\": state.error or \"Unknown error occurred\"\n",
    "        })\n",
    "        \n",
    "        return {\"agent_response\": fallback_response}\n",
    "    \n",
    "    # Create the Travel Assistant graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes for each agent and processing step\n",
    "    workflow.add_node(\"router\", router_agent)\n",
    "    workflow.add_node(\"itinerary_agent\", itinerary_agent)\n",
    "    workflow.add_node(\"flight_agent\", flight_agent)\n",
    "    workflow.add_node(\"accommodation_agent\", accommodation_agent)\n",
    "    workflow.add_node(\"information_agent\", information_agent)\n",
    "    workflow.add_node(\"response_generator\", generate_final_response)\n",
    "    workflow.add_node(\"error_handler\", handle_error)\n",
    "    \n",
    "    # Define conditional edge routing\n",
    "    def router_edges(state):\n",
    "        if state.agent_executor == \"itinerary_agent\":\n",
    "            return \"itinerary_agent\"\n",
    "        elif state.agent_executor == \"flight_agent\":\n",
    "            return \"flight_agent\"\n",
    "        elif state.agent_executor == \"accommodation_agent\":\n",
    "            return \"accommodation_agent\"\n",
    "        else:\n",
    "            return \"information_agent\"\n",
    "    \n",
    "    # Check if there's an error and route accordingly\n",
    "    def agent_edges(state):\n",
    "        if state.error is not None:\n",
    "            return \"error_handler\"\n",
    "        else:\n",
    "            return \"response_generator\"\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"router\")\n",
    "    \n",
    "    # Connect router to agents\n",
    "    workflow.add_conditional_edges(\"router\", router_edges)\n",
    "    \n",
    "    # Connect agents to next nodes\n",
    "    for agent in [\"itinerary_agent\", \"flight_agent\", \"accommodation_agent\", \"information_agent\"]:\n",
    "        workflow.add_conditional_edges(agent, agent_edges)\n",
    "    \n",
    "    # Connect to end\n",
    "    workflow.add_edge(\"response_generator\", END)\n",
    "    workflow.add_edge(\"error_handler\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    travel_assistant = workflow.compile()\n",
    "    \n",
    "    # Store the travel assistant in the user session\n",
    "    cl.user_session.set(\"travel_assistant\", travel_assistant)\n",
    "\n",
    "    # Greet the user with an introduction message\n",
    "    await cl.Message(\n",
    "        content=\"👋 Hello! I'm your Travel Assistant. I can help with travel itineraries, flight information, accommodations, and general travel advice. How can I assist you today?\",\n",
    "    ).send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "    # Get the travel assistant from the user session\n",
    "    travel_assistant = cl.user_session.get(\"travel_assistant\")\n",
    "    \n",
    "    # Get user's message\n",
    "    user_query = message.content\n",
    "    \n",
    "    # Create a message to show the user that we're thinking\n",
    "    thinking_msg = cl.Message(content=\"Thinking...\")\n",
    "    await thinking_msg.send()\n",
    "    \n",
    "    # Process the query through our travel assistant\n",
    "    result = travel_assistant.invoke({\n",
    "        \"query\": user_query,\n",
    "        \"chat_history\": [],  # We could store and use chat history for context\n",
    "    })\n",
    "    \n",
    "    # Get the final response\n",
    "    response = result.get(\"agent_response\", \"I'm sorry, I couldn't process your request. Please try again.\")\n",
    "    \n",
    "    # Update the thinking message with the actual response\n",
    "    await thinking_msg.update(content=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # To run: chainlit run app.py -w\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
